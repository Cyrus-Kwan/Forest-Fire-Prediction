{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "## Target Variable\n",
    "burned area in hectares\n",
    "\n",
    "## Models\n",
    "- Neural Network\n",
    "- Support Vector Machine\n",
    "\n",
    "## Tasks\n",
    "1. train-test split\n",
    "2. cross-validation\n",
    "3. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_ff = pd.read_csv(\"zscore_ff.csv\")\n",
    "minmax_ff = pd.read_csv(\"minmax_ff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_x = zscore_ff.iloc[:,:-1]   # Previously normalized features for model training\n",
    "zscore_y = zscore_ff.iloc[:, -1]   # Target variable for supervised learning\n",
    "\n",
    "minmax_x = minmax_ff.iloc[:,:-1]   # Previously normalized features for model training\n",
    "minmax_y = minmax_ff.iloc[:, -1]   # Target variable for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the k-fold cross-validation method\n",
    "k:int = 5\n",
    "kf = KFold(\n",
    "    n_splits=k, random_state=None, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_svm = SVR(\n",
    "    kernel=\"rbf\",\n",
    "    epsilon=1\n",
    ")\n",
    "\n",
    "minmax_svm = SVR(\n",
    "    kernel=\"rbf\",\n",
    "    epsilon=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_z:int = zscore_x.shape[1] # 12 input features for prediction\n",
    "input_dim_m:int = minmax_x.shape[1] # 12 input features for prediction\n",
    "\n",
    "z_neurons_1:int = 256                # number of neurons in first hidden layer\n",
    "z_neurons_2:int = 128                # number of neurons in second hidden layer\n",
    "z_neurons_3:int = 32                 # number of neurons in third hidden layer\n",
    "\n",
    "m_neurons_1:int = 32                # number of neurons in first hidden layer\n",
    "m_neurons_2:int = 16                # number of neurons in second hidden layer\n",
    "m_neurons_3:int = 8                 # number of neurons in third hidden layer\n",
    "\n",
    "output_dim:int = 1                  # 1 target for regression task\n",
    "\n",
    "epoch_no:int = 10                   # Number of complete pass throughs of the dataset\n",
    "batch_size:int = 8                  # Number of training samples used per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_tf = tf.keras.Sequential()\n",
    "\n",
    "zscore_tf.add(Dense(input_dim_z, activation=\"relu\"))\n",
    "zscore_tf.add(Dense(z_neurons_1, activation=\"relu\"))\n",
    "zscore_tf.add(Dense(z_neurons_2, activation=\"relu\"))\n",
    "zscore_tf.add(Dense(z_neurons_3, activation=\"relu\"))\n",
    "zscore_tf.add(Dense(output_dim, activation=\"relu\"))\n",
    "\n",
    "zscore_tf.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_tf = tf.keras.Sequential()\n",
    "\n",
    "minmax_tf.add(Dense(input_dim_z, activation=\"relu\"))\n",
    "minmax_tf.add(Dense(m_neurons_1, activation=\"relu\"))\n",
    "minmax_tf.add(Dense(m_neurons_2, activation=\"relu\"))\n",
    "minmax_tf.add(Dense(m_neurons_3, activation=\"relu\"))\n",
    "minmax_tf.add(Dense(output_dim, activation=\"relu\"))\n",
    "\n",
    "minmax_tf.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_svm_mse = []\n",
    "z_tf_mse = []\n",
    "\n",
    "m_svm_mse = []\n",
    "m_tf_mse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X=zscore_x, y=zscore_y):\n",
    "    X_train, X_test = zscore_x.iloc[train_index], zscore_x.iloc[test_index]\n",
    "    y_train, y_test = zscore_y.iloc[train_index], zscore_y.iloc[test_index]\n",
    "    \n",
    "    # Fit the model to the current split\n",
    "    # Each split is independent from another\n",
    "    zscore_svm.fit(\n",
    "        X=X_train, y=y_train\n",
    "    )\n",
    "    zscore_tf.fit(\n",
    "        X_train, y_train, epochs=epoch_no, batch_size=batch_size, verbose=0\n",
    "    )\n",
    "\n",
    "    # Make predictions based on the current train-test split\n",
    "    svm_pred = zscore_svm.predict(X=X_test)\n",
    "    model_pred = zscore_tf.predict(X_test)\n",
    "\n",
    "    # Add r-square score to list for each split\n",
    "    z_svm_mse.append(\n",
    "        mean_squared_error(y_true=y_test, y_pred=svm_pred)\n",
    "    )\n",
    "    z_tf_mse.append(\n",
    "        mean_squared_error(y_true=y_test, y_pred=model_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X=minmax_x, y=minmax_y):\n",
    "    X_train, X_test = minmax_x.iloc[train_index], minmax_x.iloc[test_index]\n",
    "    y_train, y_test = minmax_y.iloc[train_index], minmax_y.iloc[test_index]\n",
    "    \n",
    "    # Fit the model to the current split\n",
    "    # Each split is independent from another\n",
    "    minmax_svm.fit(\n",
    "        X=X_train, y=y_train\n",
    "    )\n",
    "    minmax_tf.fit(\n",
    "        X_train, y_train, epochs=epoch_no, batch_size=batch_size, verbose=0\n",
    "    )\n",
    "\n",
    "    # Make predictions based on the current train-test split\n",
    "    svm_pred = minmax_svm.predict(X=X_test)\n",
    "    model_pred = minmax_tf.predict(X_test)\n",
    "\n",
    "    # Add r-square score to list for each split\n",
    "    m_svm_mse.append(\n",
    "        mean_squared_error(y_true=y_test, y_pred=svm_pred)\n",
    "    )\n",
    "    m_tf_mse.append(\n",
    "        mean_squared_error(y_true=y_test, y_pred=model_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7465310953804492, 2.1626130817612994, 2.081075633542362, 1.9419463463482372, 1.742291820180092]\n",
      "[2.0299215098015564, 2.1602199343883592, 1.8737912769220086, 1.2990418026516666, 0.9156373540936971]\n",
      "SVM MSE: 1.934891595442488\n",
      "NN MSE 1.6557223755714578\n",
      "SVM MSE hectares 3.018882217286757\n",
      "NN MSE hectares 2.6209946802203428\n"
     ]
    }
   ],
   "source": [
    "# MSE for each cross-validation instance\n",
    "print(z_svm_mse)\n",
    "print(z_tf_mse)\n",
    "\n",
    "# Average MSE for all cross-validation instances\n",
    "print(\"SVM MSE:\", sum(z_svm_mse)/len(z_svm_mse))\n",
    "print(\"NN MSE\", sum(z_tf_mse)/len(z_tf_mse))\n",
    "\n",
    "# Average error in hectares\n",
    "svm_x = np.sqrt(sum(z_svm_mse)/len(z_svm_mse))\n",
    "tf_x = np.sqrt(sum(z_tf_mse)/len(z_tf_mse))\n",
    "\n",
    "print(\"SVM MSE hectares\", np.exp(svm_x)-1)\n",
    "print(\"NN MSE hectares\", np.exp(tf_x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7431117127044222, 1.8644533387275064, 1.332753545381297, 2.397413218151492, 2.319460713329401]\n",
      "[1.747680216206736, 1.9529349962853657, 1.3042910824822966, 2.256343431783386, 2.2439228223885443]\n",
      "SVM MSE: 1.9314385056588237\n",
      "NN MSE 1.901034509829266\n",
      "SVM MSE hectares 3.0138947609269477\n",
      "NN MSE hectares 2.9700555232197954\n"
     ]
    }
   ],
   "source": [
    "# MSE for each cross-validation instance\n",
    "print(m_svm_mse)\n",
    "print(m_tf_mse)\n",
    "\n",
    "# Average MSE for all cross-validation instances\n",
    "print(\"SVM MSE:\", sum(m_svm_mse)/len(m_svm_mse))\n",
    "print(\"NN MSE\", sum(m_tf_mse)/len(m_tf_mse))\n",
    "\n",
    "# Average error in hectares\n",
    "svm_x = np.sqrt(sum(m_svm_mse)/len(m_svm_mse))\n",
    "tf_x = np.sqrt(sum(m_tf_mse)/len(m_tf_mse))\n",
    "\n",
    "print(\"SVM MSE hectares\", np.exp(svm_x)-1)\n",
    "print(\"NN MSE hectares\", np.exp(tf_x)-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
